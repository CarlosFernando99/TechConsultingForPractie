"""
Author: Carlos Fernando Arriola Gutierrez
Date: January 2026
Region: ap-south-1 (Mumbai)
Final code that worked
"""

# ============================================================================
# LAMBDA 2: CONSUMER LAMBDA - Kinesis to S3 Bronze
# ============================================================================
# Purpose:
#   - Consume records from a Kinesis Data Stream
#   - Decode and enrich each record
#   - Persist raw data into S3 (Bronze layer)
#   - Track ingestion metadata in DynamoDB
#
# Trigger:
#   - Kinesis Data Stream (batch-based invocation)
#
# Architecture role:
#   - Streaming consumer
#   - Bronze ingestion layer
#
# Timeout: 120 seconds
# Memory: 512 MB
# ============================================================================

import json
import base64
import boto3
import logging
from datetime import datetime
from io import BytesIO

# ----------------------------------------------------------------------------
# AWS clients/resources
# ----------------------------------------------------------------------------
s3_client = boto3.client('s3', region_name='ap-south-1')
dynamodb = boto3.resource('dynamodb', region_name='ap-south-1')
sqs_client = boto3.client('sqs', region_name='ap-south-1')
cloudwatch = boto3.client('cloudwatch', region_name='ap-south-1')

# ----------------------------------------------------------------------------
# Logging configuration
# ----------------------------------------------------------------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ----------------------------------------------------------------------------
# Lambda handler
# ----------------------------------------------------------------------------
def lambda_handler_consumer(event, context):
    """
    Consumer Lambda: Reads records from Kinesis and writes them to S3 Bronze layer

    High-level flow:
    1. Receive batch of records from Kinesis
    2. Decode Base64 payloads
    3. Enrich records with ingestion metadata
    4. Write batch as JSON Lines file to S3 (Bronze)
    5. Track ingestion metadata in DynamoDB
    6. Emit CloudWatch metrics

    Args:
        event (dict): Kinesis event payload
        context (LambdaContext): Runtime metadata

    Returns:
        dict: HTTP-style response with processing summary
    """

    try:
        # --------------------------------------------------------------------
        # Step 1: Configuration / constants
        # --------------------------------------------------------------------
        s3_bucket = "bdtraining1811"
        s3_prefix = "bronze"
        table_name = "incremental_load_tracking"
        dlq_url = "https://sqs.ap-south-1.amazonaws.com/430006376054/data-pipeline-dlq"

        logger.info(f"Consumer Lambda started. Processing {len(event['Records'])} records")

        # --------------------------------------------------------------------
        # Step 2: Initialize counters and buffers
        # --------------------------------------------------------------------
        records_to_write = []     # Buffer for valid records
        processed_records = 0
        failed_records = 0

        # --------------------------------------------------------------------
        # Step 3: Process each record from Kinesis
        # --------------------------------------------------------------------
        for record in event['Records']:
            try:
                # Kinesis data is Base64-encoded → decode it
                payload_bytes = base64.b64decode(record['kinesis']['data'])

                # Convert bytes → JSON object
                record_data = json.loads(payload_bytes)

                logger.info(f"Processing record number: {record_data.get('record_number')}")

                # Enrich record with ingestion metadata
                record_data['s3_ingestion_timestamp'] = datetime.now().isoformat()
                record_data['kinesis_shard_id'] = record.get('kinesis', {}).get('shardId') or record.get('eventID', '').split(':')[0]
                record_data['kinesis_sequence_number'] = record.get('kinesis', {}).get('sequenceNumber')

                # Add record to batch buffer
                records_to_write.append(record_data)
                processed_records += 1

            except Exception as e:
                # Individual record failure should not break the batch
                logger.error(f"Failed to process Kinesis record: {str(e)}")
                failed_records += 1

                # Send error to DLQ for later inspection
                try:
                    sqs_client.send_message(
                        QueueUrl=dlq_url,
                        MessageBody=json.dumps({
                            "error": str(e),
                            "error_type": "CONSUMER_RECORD_PROCESSING_ERROR",
                            "timestamp": datetime.now().isoformat()
                        })
                    )
                except Exception:
                    pass

        # --------------------------------------------------------------------
        # Step 4: Write batch to S3 Bronze layer
        # --------------------------------------------------------------------
        if records_to_write:
            try:
                now = datetime.now()

                # Partitioned S3 path (year/month/day/hour)
                s3_key = (
                    f"{s3_prefix}/"
                    f"{now.year}/{now.month:02d}/{now.day:02d}/{now.hour:02d}/"
                    f"users_{now.isoformat()}.json"
                )

                # Convert batch to JSON Lines format (one JSON per line)
                jsonl_content = "\n".join(
                    [json.dumps(record) for record in records_to_write]
                )

                logger.info(
                    f"Writing {len(records_to_write)} records to "
                    f"s3://{s3_bucket}/{s3_key}"
                )

                # Upload object to S3
                s3_client.put_object(
                    Bucket=s3_bucket,
                    Key=s3_key,
                    Body=jsonl_content.encode('utf-8'),
                    ContentType='application/json'
                )

                logger.info(f"Successfully wrote Bronze data to S3")

                # ----------------------------------------------------------------
                # Step 5: Track ingestion in DynamoDB
                # ----------------------------------------------------------------
                table = dynamodb.Table(table_name)

                table.put_item(
                    Item={
                        'load_id': f"load_{now.isoformat()}",
                        'load_timestamp': now.isoformat(),
                        'layer': 'bronze',
                        'record_count': len(records_to_write),
                        's3_path': s3_key,
                        'status': 'SUCCESS',
                        'processed_records': processed_records,
                        'failed_records': failed_records
                    }
                )

                logger.info("DynamoDB load tracking updated")

            except Exception as e:
                logger.error(f"Failed to write to S3 or DynamoDB: {str(e)}")

                # Send batch-level failure to DLQ
                try:
                    sqs_client.send_message(
                        QueueUrl=dlq_url,
                        MessageBody=json.dumps({
                            "error": str(e),
                            "error_type": "CONSUMER_S3_WRITE_ERROR",
                            "timestamp": datetime.now().isoformat()
                        })
                    )
                except Exception:
                    pass

        # --------------------------------------------------------------------
        # Step 6: Emit CloudWatch metrics
        # --------------------------------------------------------------------
        cloudwatch.put_metric_data(
            Namespace='DataPipeline',
            MetricData=[
                {
                    'MetricName': 'ConsumerProcessedRecords',
                    'Value': processed_records,
                    'Unit': 'Count'
                },
                {
                    'MetricName': 'ConsumerFailedRecords',
                    'Value': failed_records,
                    'Unit': 'Count'
                }
            ]
        )

        # --------------------------------------------------------------------
        # Step 7: Success response
        # --------------------------------------------------------------------
        logger.info(
            f"Consumer Lambda completed. "
            f"Processed: {processed_records}, Failed: {failed_records}"
        )

        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Data successfully written to S3 Bronze layer',
                'processed_records': processed_records,
                'failed_records': failed_records,
                'timestamp': datetime.now().isoformat()
            })
        }

    except Exception as e:
        # Critical failure (Lambda-level)
        logger.error(f"Critical error in Consumer Lambda: {str(e)}")

        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
        }
